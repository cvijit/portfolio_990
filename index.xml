<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to my Data World on Vijit Laxman Chekkala</title>
    <link>https://cvijit.github.io/portfolio_990/</link>
    <description>Recent content in Welcome to my Data World on Vijit Laxman Chekkala</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Apr 2020 11:00:59 -0400</lastBuildDate>
    
	<atom:link href="https://cvijit.github.io/portfolio_990/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bone Age Assessment based on pre-trained convolutional neural networks</title>
      <link>https://cvijit.github.io/portfolio_990/post/project-3/</link>
      <pubDate>Wed, 15 Apr 2020 11:00:59 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/post/project-3/</guid>
      <description>Bone Age Assessment (BAA) is a medical test undertaken to identify the difference between a child’s skeletal bone age and chronological age, and the disparity between the results will often suggest either hormonal or skeletal system development anomalies . The estimation of pediatric bone age using the radiological dataset comparing with the standard hand atlas can help in determining hormonal or endocrinological diseases and forethought on the pediatric orthopedic surgeries .</description>
    </item>
    
    <item>
      <title>Predicting Poverty Level with Business Strategy and Machine Learning Techniques</title>
      <link>https://cvijit.github.io/portfolio_990/post/project-6/</link>
      <pubDate>Mon, 13 Apr 2020 11:15:58 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/post/project-6/</guid>
      <description>The primary goal of millennium and sustainable development goals is to eradicate poverty at a global level. To alleviate poverty, business strategy is followed which is gained from literature reviews and surveys. The dataset is taken from Kaggle and it consists of 10,000 instances with 143 variables. Data preprocessing is done based on the business required and the implementation part is done by building machine learning models. The data preprocessing steps were implemented in the R programming language and the model implementation part was done in Python programming language.</description>
    </item>
    
    <item>
      <title>Development of machine learning based models on independent datasets</title>
      <link>https://cvijit.github.io/portfolio_990/post/project-4/</link>
      <pubDate>Wed, 18 Dec 2019 11:13:32 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/post/project-4/</guid>
      <description>Prediction of air quality in an Italian city using multiple linear regression and support vector regression. Online shoppers purchasing intention using decision tree and random forest. Predicting the price from listings summary of Airbnb in Berlin using K nearest neighbours algorithm.  GitHub Repository</description>
    </item>
    
    <item>
      <title>Applying business intelligence to alpha group insurance company focusing on revenue statistics</title>
      <link>https://cvijit.github.io/portfolio_990/post/project-5/</link>
      <pubDate>Sun, 08 Dec 2019 11:14:48 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/post/project-5/</guid>
      <description>AlfaStrakhovanie Group remains one of the largest insurance companies in Russia, offering a wide range of insurance services for clients not excluding life insurance. Here it is focused on the sector within the company that provides auto insurance. Power BI is used to make dashboards and visualisations which will provide detailed insights on the company’s performance. CRM which is a tool to manage the relationship between a company and its active or inactive customers will be used in this study as a means to track customer service responsiveness, which can be understood by analysing the cases and providing solutions to them.</description>
    </item>
    
    <item>
      <title>Investigate leading causes of deaths based on drug usage and violation of crime. </title>
      <link>https://cvijit.github.io/portfolio_990/post/project-2/</link>
      <pubDate>Sun, 10 Nov 2019 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/post/project-2/</guid>
      <description>The main aim of the project is to find four parallel data sets on a specific domain and performing exploratory data analysis. Insights produced from this process are visualised using different plots. After thorough research, we had summarised the root cause of deaths due to factors like shooting incidents, drug use, criminal courts and other leading causes of deaths in the United States of America. Also, we estimated the death rates as per factors like gender, race, targeted locations, and boroughs.</description>
    </item>
    
    <item>
      <title>Dataset Size</title>
      <link>https://cvijit.github.io/portfolio_990/blogs/blog3/</link>
      <pubDate>Sun, 09 Apr 2017 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/blogs/blog3/</guid>
      <description>The size of the dataset can have an effect on the machine learning algorithm applied. However it is unpredictable about how the dataset size can have an impact on the model. Sometimes a machine learning algorithm can produce valuable information and predict good results from 100 samples of data points. In other cases, it becomes difficult to extract information from even 10,000 samples of data points. The information inside the data sets matters for the machine learning algorithms rather than the size of the dataset.</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction Techniques</title>
      <link>https://cvijit.github.io/portfolio_990/blogs/blog1/</link>
      <pubDate>Sun, 09 Apr 2017 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/blogs/blog1/</guid>
      <description>Dimensionality reduction - If a data mining/machine learning project is considered to be complex, then it is sure that the data types in such projects consist of image and video data or multi-class classification data points. This data which acts as input variables in the datasets are called dimensions and the dimensionality reduction technique is carried out to reduce them based on their importance. Some basic dimensionality reduction techniques are to deal with the missing values, low variance in column values, high correlation between the columns.</description>
    </item>
    
    <item>
      <title>Ensemble Approaches</title>
      <link>https://cvijit.github.io/portfolio_990/blogs/blog5/</link>
      <pubDate>Sun, 09 Apr 2017 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/blogs/blog5/</guid>
      <description>In machine learning, no one algorithm alone can provide good results in prediction. However better predictions can be generated if multiple learning algorithms are used, called ensemble learning. Therefore using a combination of base learners are used to get higher accuracy in predicting the target variable. The base learner differs as every model implemented will have different algorithms, internal parameters, training sets and different solutions based on the business problem.</description>
    </item>
    
    <item>
      <title>Feature Engineering</title>
      <link>https://cvijit.github.io/portfolio_990/blogs/blog2/</link>
      <pubDate>Sun, 09 Apr 2017 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/blogs/blog2/</guid>
      <description>The best way to create a good prediction model is to make the best use of the data that is available. In a machine learning project, the results depend on many factors like the business problem, the data available, features extracted, defined algorithms and the evaluation method. The most important step in all these are the features extracted based on their importance or the business problem. If wrong parameters or a low optimal model is implemented on good features, it can still produce good results because of the structure of the data.</description>
    </item>
    
    <item>
      <title>Unsupervised Deep Learning with Artificial Neural Networks</title>
      <link>https://cvijit.github.io/portfolio_990/blogs/blog4/</link>
      <pubDate>Sun, 09 Apr 2017 10:58:08 -0400</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/blogs/blog4/</guid>
      <description>Unsupervised Deep Learning with Artificial Neural Networks (ANN): In the context of unsupervised learning, ANN helps discover itself by forming patterns, clusters or based on the relationships of interests. The networks have some self-organizing property and the all the results from the previous networks are evaluated.
  Two categories of ANN approaches are: Competitive learning where the decision made by the neural networks belong to either clustering or classification and Hebbian learning are more directed towards the components of the input data in a project.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://cvijit.github.io/portfolio_990/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cvijit.github.io/portfolio_990/contact/</guid>
      <description>To get to know more about me, you can follow me the below mentioned social media profiles:
   Platform URL     LinkedIn: https://www.linkedin.com/in/vijit588/   Email: vijit588@gmail.com    </description>
    </item>
    
  </channel>
</rss>